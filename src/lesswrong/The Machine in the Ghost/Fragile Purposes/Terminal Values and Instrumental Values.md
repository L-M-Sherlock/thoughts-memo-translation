# 终极价值与工具性价值

[Terminal Values and Instrumental Values](https://www.readthesequences.com/Terminal-Values-And-Instrumental-Values)

❦

在纯粹本能的层面上，任何人类规划者的行为都仿佛他们区分手段与目的。想吃巧克力？Publix 超市有巧克力。你沿着 Washington Ave 向南开 1 英里就能到超市。你上了车就能开车。你打开车门就能上车。你有车钥匙就能开车门。于是你把车钥匙塞进口袋，准备出门……

……忽然，广播里传来消息：一场地震摧毁了当地那家 Publix 里的所有巧克力。好吧，如果那里没有巧克力，就没必要开车去 Publix；如果你不打算开车去任何地方，就没必要上车；如果你不打算开车，就没必要把车钥匙放在口袋里。于是你把车钥匙从口袋里掏出来，打电话给本地披萨店，让他们送一份巧克力披萨来。嗯，真好吃。

**我很少看**到人们把自己制定的计划搞丢。人们通常不会在知道巧克力没了的情况下还开车去超市。但我也注意到，当人们开始明确地谈论目标系统，而不只是想要某样东西；开始「提到」目标，而不是「使用」目标时，他们往往会困惑。人类是[擅长规划的专家，而不是擅长研究规划的专家](http://www.overcomingbias.com/2007/04/expert_at_versu.html)，不然世界上会多出许多 AI 开发者。

我尤其注意到：当人们——在抽象的哲学讨论中，而不是在日常生活里——思考手段与目的的区分；更形式化地说，思考「工具性价值」（instrumental values）与「终极价值」（terminal values）之间的区分时，他们会陷入困惑。

在我看来，问题的一部分在于：人类心智用一套相当临时拼凑的系统来追踪自己的目标——它能用，但不干净。英语并不体现手段与目的之间的鲜明区分：「我想救我妹妹的命」与「我想给我妹妹注射青霉素」使用的是同一个词 want。

我们能否只用英语来描述那个正在丢失的区分？

先试着来一版：

「工具性价值」之所以值得追求，严格取决于它们所预期的后果。「我想给我妹妹注射青霉素」，不是因为「充满青霉素的妹妹」本身是某种内在的好，而是因为我预期青霉素会治好她那种会吞噬血肉的肺炎。如果你预期注射青霉素会像《绿野仙踪》里的西方坏女巫那样把你妹妹融化成一滩水，你同样会竭尽全力让她远离青霉素。

「终极价值」之所以值得追求，并不取决于其他后果：「我想救我妹妹的命」与我是否预期她之后会不会被注射青霉素无关。

这第一版显然有缺陷。如果救我妹妹的命会导致地球被黑洞吞噬，那么我会走开哭一会儿，但我不会给她注射青霉素。这是否意味着：救我妹妹的命并不是一种「终极」或「内在」价值，因为从理论上讲它取决于后果？我救她的命是否**仅仅**是因为我相信之后黑洞不会吞掉地球？常识会说，事情并不是这样运作的。

所以先别用英语。我们可以建立一个决策系统的数学描述：在其中，终极价值与工具性价值是两种分离且互不兼容的类型——就像在一种没有自动类型转换的编程语言里，整数与浮点数是两种互不兼容的类型。

一个理想的贝叶斯决策系统，只用四个元素就可以搭建：

-   Outcomes（结果）：type Outcome\[\]

    -   可能结果的列表

    -   { 妹妹活着，妹妹死去 }

-   Actions（行动）：type Action\[\]

    -   可能行动的列表

    -   { 注射青霉素，不注射青霉素 }

-   Utility_function（效用函数）：type Outcome ⇒ Utility

    -   将每个结果映射到一个效用的效用函数

    -   （效用可表示为介于负无穷与正无穷之间的实数）

    -   ![formula: { sister lives ↦ 1 ; sister dies ↦ 0 }](https://www.readthesequences.com/wiki/uploads/TerminalValuesAndInstrumentalValues_formula_1.svg "formula: { sister lives ↦ 1 ; sister dies ↦ 0 }")

-   Conditional_probability_function（条件概率函数）：type Action ⇒ (Outcome ⇒ Probability)

    -   将每个行动映射到「结果上的概率分布」的条件概率函数

    -   （概率可表示为 0 到 1 之间的实数）

    -   ![formula: { administer penicillin ↦ ( sister lives ↦ 0.9 ; sister dies ↦ 0.1 ) ; don't administer penicillin ↦ (sister lives ↦ 0.3 ; sister dies ↦ 0.7 ) }](https://www.readthesequences.com/wiki/uploads/TerminalValuesAndInstrumentalValues_formula_2.svg "formula: { administer penicillin ↦ ( sister lives ↦ 0.9 ; sister dies ↦ 0.1 ) ; don't administer penicillin ↦ (sister lives ↦ 0.3 ; sister dies ↦ 0.7 ) }")

而决策系统本身呢？

-   Expected_Utility（期望效用）：Action A ⇒ (Sum O in Outcomes: Utility(O) \* Probability(O\|A))

    -   一个行动的「期望效用」，等于对所有结果求和：该结果的效用乘以在该行动之下该结果发生的条件概率。

    -   ![formula: { EU(administer penicillin) = 0.9 ; EU(don't administer penicillin) ↦ 0.3 }](https://www.readthesequences.com/wiki/uploads/TerminalValuesAndInstrumentalValues_formula_3.svg "formula: { EU(administer penicillin) = 0.9 ; EU(don't administer penicillin) ↦ 0.3 }")

-   Choose（选择）：⇒ (Argmax A in Actions: Expected_Utility(A))

    -   选择一个「期望效用」最大的行动。

    -   { return: administer penicillin }

对每一种行动，计算所有可能随之发生的后果的条件概率；然后把这些后果的效用乘以其条件概率，再加总起来。最后选出最佳行动。

这是一个在数学上很简单的决策系统草图。但在现实世界中，这并不是一种高效的决策计算方式。

比如说，如果你需要一连串行动来执行一个计划，该怎么办？这个形式体系很容易表示：让每一个 Action 代表一整段行动**序列**。但这会制造出一个指数级巨大的空间，就像你用 100 个字母能打出的所有句子构成的空间。举一个简单例子：如果第一步的某个可选行动是「把自己的脚开枪打掉」，人类规划者通常会判断这是个坏主意——把所有以这个行动开头的序列都排除掉。但我们把这种结构从表示里抹平了。我们没有行动序列，只有扁平的「行动」。

所以，没错，确实有「一些小小的复杂性」。显然如此，不然我们早就冲出去用这种方法造出真正的 AI 了。从这个意义上说，它与贝叶斯概率理论本身非常相像。

但这恰恰是那种场景：在加入任何高谈阔论的复杂性之前，先考虑荒谬地简单版本，是一个出奇地好主意。

**想想这样**一位哲学家，他断言：「我们所有人归根结底都是自私的；我们只关心自己的心智状态。那位声称关心儿子福祉的母亲，其实是想要相信自己的儿子过得很好——这个信念才让母亲快乐。她帮助儿子，是为了自己的快乐，而不是为了儿子。」你说：「好吧，假设母亲为了把儿子从迎面驶来的卡车前推开而牺牲了自己的生命。这不会让她快乐，只会让她死。」哲学家结结巴巴了一会儿，然后回答：「但她仍然这么做，是因为她看重那个选择胜过其他选择——因为她把一种重要感附着在那个决策上。」

于是你说：

类型错误：找不到 Expected_Utility ⇒ Utility 的构造器。

请允许我解释这一回应。

即使是我们的简单形式体系，也展示了期望效用与效用之间的鲜明区分：期望效用是**行动**所具有的东西；效用是结果所具有的东西。没错，你可以把效用与期望效用都映射到实数。但这就像你观察到风速与温度都可以映射到实数一样。这并不能说明它们是同一回事。

哲学家首先论证：你的所有 Utilities 都必须对应于由你的心智状态构成的 Outcomes。如果这是真的，那么你的智能将作为一种引擎，把未来导向你快乐的区域。未来状态只会因你的心智状态而区分；在任何两个未来中，只要你的心智状态相同，你就会对它们无差别。

而你确实不太可能为了拯救他人而牺牲自己的生命。

**当我们反**驳说人们有时确实会牺牲生命时，哲学家的回应转而去讨论行动上的 Expected Utilities：「她附着在那个决策上的重要感」。这是一次巨大跳跃，它理应让我们愤怒得从椅子上跳起来。试图把一个 Expected_Utility 转换成 Utility，会在我们的编程语言里引发直接错误。但用英语说出来，听起来却都一样。

我们的简单决策系统会选择 Expected_Utility 最高的选项，但这并没有对「它把未来导向哪里」说任何话。它没有对决策者赋予何种效用说任何话，也没有对哪些现实世界结果会因此发生说任何话。它没有对心智作为引擎的功能说任何话。

一个物理行动的物理原因，是一个认知状态；在我们的理想决策者中，那就是 Expected_Utility，而这个期望效用是通过评估「想象后果」上的效用函数计算出来的。要救你儿子的命，你必须先想象「你儿子活下来」这一事件，而这种想象并不是事件本身。它是一种「**引用**」，就像「snow」与雪之间的区别。但这并不意味着：引号里的东西本身必须是一个认知状态。如果你选择了那个会导向你用「我的儿子还活着」所表征的未来的行动，那么你就作为一个引擎，把未来导向了「你的儿子还活着」的区域——而不是把未来导向「你表征了句子『我的儿子还活着』」的区域。要把未来导向后者，你的效用函数就必须在输入「 “my son is still alive” 」时返回高效用——那是一种对引用的引用，是你想象自己在想象。把食谱磨成粉拌进面糊里，是做不出好蛋糕的。

这就是为什么：先考虑简单决策系统是有帮助的。往系统里混入足够多的复杂性，原本清晰的区分就会变得更难看见。

那么现在，让我们看看一些复杂性。显然，Utility function（把 Outcomes 映射到 Utilities）旨在形式化我先前称为「终极价值」的东西——不取决于其后果的价值。那么，如果「救你妹妹的命」会导致地球被黑洞吞噬，这种情况怎么办？在我们的形式体系里，我们把这种可能性抹平了。Outcomes 不会导向 Outcomes，只有 Actions 才会导向 Outcomes。你妹妹从肺炎中恢复、然后地球被黑洞吞噬，这会被抹平成一个单一的「可能结果」。

在这个简单形式体系里，「工具性价值」又在哪里？实际上，它们完全消失了！你看，在这个形式体系里，行动会在没有任何中间事件的情况下直接通向结果。这里没有「扔出一块石头，石头飞过空气，打落树枝上的苹果，于是苹果掉到地上」这样的概念。扔石头就是 Action；而它会直接导向「苹果躺在地上」这一 Outcome——根据那条把 Action 直接变成 Outcomes 上概率分布的条件概率函数。

为了真正计算条件概率函数，以及为了分别考虑「妹妹得肺炎」与「黑洞吞噬地球」的效用，我们必须表示因果性的网络结构——事件如何导向其他事件。

然后，工具性价值就会开始回归。如果因果网络足够规则，你就能找到一个状态 *B*：无论你如何达成 *B*，它都倾向于导向 *C*。那么如果你因为某种原因想达成 *C*，你就可以先高效地找出一个能通向 *C* 的 *B*，再找出一个能通向 *B* 的 *A*。这就是「工具性价值」这一现象——*B* 会因为它导向 *C* 而具有「工具性价值」。状态 *C* 本身可能是被终极价值所重视的——它是对整体结果的效用函数里的一个项。或者 *C* 也可能只是工具性价值——一个没有被效用函数直接赋值的节点。

在这个形式体系里，工具性价值纯粹是为了高效计算计划的辅助物。在不存在这种规则性的地方，它可以、也应该被丢弃。

举例来说，假设某个特定的 *B* 并不会导向 *C*。你还会选择一个导向该 *B* 的 *A* 吗？或者别管抽象哲学了：如果你想去超市买巧克力，而你想开车去超市，并且你需要进入你的车，你会用蒸汽铲把车门直接撕掉来进入车里吗？（不会。）用我们程序员的话说，工具性价值是一种「渗漏的抽象」；有时你必须丢掉缓存的价值，重新计算实际的期望效用。要做到高效而不自杀，一部分就在于：当方便的捷径失效时，你要能注意到。尽管这个形式体系确实会导出工具性价值，但它只会在必要的规则性存在之处导出它，并且严格把它作为计算中的一种便利捷径。

但如果你在理解简单版本之前就把形式体系复杂化，你可能会开始认为：工具性价值在某种意义上（甚至在规范意义上）有一种奇怪的自我生命。也就是说，一旦你说 *B* 通常是好的，因为它导向 *C*，你就承诺自己即便在缺少 *C* 的情况下也永远要追求 *B*。人们在抽象哲学里会犯这种错误，尽管他们在现实生活中绝不会用蒸汽铲撕开车门。你可能会开始认为：无法发展出一个[只最大化包容性遗传适应度的结果主义者](https://www.readthesequences.com/ThouArtGodshatter)，因为除非你把「吃饭」纳入一个显式的终极价值，否则它会饿死。人们会犯这种错误，尽管他们绝不会因为害怕自己若没有「开车门」这一终极价值就会被困在车外，而整天站着不停开车门。

工具性价值存在于条件概率函数（的网络结构）中。这使得：在效用函数固定时，工具性价值严格依赖于事实信念。如果我相信青霉素会导致肺炎，并且相信缺少青霉素会治好肺炎，那么我对青霉素的工具性价值评估就会从高变低。改变事实信念——改变那条把行动与被相信的后果关联起来的条件概率函数——工具性价值就会随之整体改变。

在道德争论中，有些分歧是关于工具性后果的，有些分歧是关于终极价值的。如果你的辩论对手说禁枪会降低犯罪，而你说禁枪会提高犯罪，那么你们在更高层的工具性价值上是一致的（犯罪不好），但你们在「哪些中间事件会导向哪些后果」上意见不同。但我不认为，关于女性割礼的争论，真的是一场关于「如何最好地实现一个共同价值：公平对待女性或让女性幸福」的事实争论。

在愤怒的争论里，这个重要区分常常被直接冲进马桶。那些在事实问题上有分歧、但共享价值的人，会各自断定对方一定是反社会者。仿佛你憎恨的敌人——控枪/拥枪倡议者——真的想杀人；这在真实心理学上应当是[不太可信的](https://www.readthesequences.com/AreYourEnemiesInnatelyEvil)。

我担心，人类大脑并没有强类型化终极道德信念与工具性道德信念之间的区分。作为道德信念，「我们应该禁枪」与「我们应该拯救生命」并不会让人感觉有什么不同，就像视觉与听觉给人的感觉不同那样。尽管人类目标系统以诸多方式把眼前的一切都搅复杂了，但它偏偏把这一个区分也坍缩成一团「具有条件价值之物」的混杂体。

为了提取终极价值，我们必须检查这团混杂的有价值之物，试图弄清哪些东西的价值来自别处。这是一个艰难的项目！如果你说你想禁枪是为了降低犯罪，你可能需要过一会儿才意识到：「降低犯罪」并不是终极价值，它是一个更高层的工具性价值；它通过链接指向人类生命与人类幸福这些终极价值。然后，主张拥枪的人也可能同样链接到「降低犯罪」这一更高层的工具性价值，同时还链接到「自由」这一价值；对他们而言，自由可能是终极价值，也可能又是一种工具性价值……

我们无法把那张由其他价值推导而来的完整价值网络打印出来。我们甚至很可能连价值如何来到那里的完整历史都没有存储。通过考虑恰当的道德两难，例如「如果 *Y*，你还会做 *X* 吗？」我们往往能推断我们的价值从何而来。但即便这个项目本身也充满陷阱：误导性的两难、漏洞百出的哲学论证。我们不知道自己的价值是什么，也不知道它们从何而来；除非开展容易出错的认知考古项目，否则我们无从得知。仅仅是形成「终极价值」与「工具性价值」的有意识区分，并跟踪它的含义、正确使用它，就已经是艰苦的脑力劳动。只有检查这个简单形式体系，我们才能看出：原则上这本该有多容易。

更不用说人类奖赏系统的其他所有复杂性了——强化学习架构的整套使用方式，以及吃巧克力令人愉悦、预期吃巧克力也令人愉悦，但那又是不同种类的愉悦……

不过，我并不会对这团混乱抱怨太多。

对自己的价值一无所知也许并不总是好玩，但至少不无聊。