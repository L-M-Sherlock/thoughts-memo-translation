# 机器中的幽灵

[Ghosts in the Machine](https://www.readthesequences.com/Ghosts-In-The-Machine)

❦

人们听到「友好型 AI」时，经常会说——这是最常见的三种初始反应之一：

「哦，你可以试着告诉 AI 要友好，但如果 AI 能修改自己的源代码，它就会把你试图施加在它身上的任何约束都删掉。」

而**那个**决策又从何而来？

它是从因果之外闯入的吗？它不是从最初写下的源代码出发、在一条合法的因果链中产生的效应吗？AI 是它自身自由意志的[最终源头](https://www.greaterwrong.com/lw/rc/the_ultimate_source/)吗？

**友好型** AI 并不是一个自私的 AI，被一个额外加装的「良心模块」约束；那个良心模块会覆盖 AI 的自然冲动并告诉它该做什么。你只要把良心造出来，那就是 AI。如果你有一段程序，能够计算出 AI 应该做出哪个决策，那么你就已经完工了。[责任立刻到此为止。](https://www.greaterwrong.com/lw/ra/causality_and_moral_responsibility/)

在这里，我要稍微停一下，引用一些来自 Computer Stupidities 网站（Programming 子主题）上的案例研究。（我不会贴链接，因为那是一个可怕的时间黑洞；如果你胆子够大，可以自己 Google。）

> 我曾辅导正在上计算机编程课的大学生。有些人不理解计算机不是有感知的。有人在他们的 Pascal 程序里用注释写下非常详细的解释，比如：「现在我需要你把这些字母显示到屏幕上。」我问其中一个人，为什么要写这些注释。他回答：「不然计算机怎么理解我想让它做什么？」显然，他们会以为：既然他们自己都看不懂 Pascal，那计算机也一样看不懂。

>

> ------------------------------------------------------------------------

>

> 上大学时，我曾在学校的数学辅导室当家教。有个学生来找我，因为他的 BASIC 程序跑不起来。他在上入门课，作业是写一个程序：根据你要给多少人烤燕麦曲奇，计算配方。我看了他的程序，大概是这样的：

>

> 10 将烤箱预热到 350

>

> 20 把所有材料放进一个大搅拌碗里

>

> 30 搅拌至顺滑

>

> ------------------------------------------------------------------------

>

> 曾有一位编程入门学生让我看他的程序，帮他找出为什么一次简单计算的结果总是输出 0。我看了程序，问题非常明显：

>

> begin

>

> read("苹果数量", apples)

>

> read("胡萝卜数量", carrots)

>

> read("1 个苹果的价格", a_price)

>

> read("1 根胡萝卜的价格", c_price)

>

> write("苹果合计", a_total)

>

> write("胡萝卜合计", c_total)

>

> write("总计", total)

>

> total = a_total + c_total

>

> a_total = apples \* a_price

>

> c_total = carrots \* c_price

>

> end

>

> 我：「你的程序不可能在计算出正确结果之前，就先把正确结果打印出来。」

>

> 他：「啊？正确答案是什么这件事是合乎逻辑的，计算机应该自己把指令按正确顺序重排。」

人们会本能地想象「给 AI 编程」这一场景。他们会把它映射到一个看似相似的人类行为：告诉一个人该做什么。仿佛「程序」是在给机器里坐着的小幽灵下指令；那只幽灵会审阅你的指令，然后决定它喜不喜欢、愿不愿意照做。

**并不存在**一个幽灵来审阅指令、并决定如何遵循它们。程序本身就是 AI。

**这并不意**味着那个幽灵会像[精灵](https://www.readthesequences.com/TheHiddenComplexityOfWishes)一样满足你的一切愿望；也不意味着那个幽灵会像极其温顺的奴隶一样，以你希望的方式完成你想要的一切。这意味着：至少在启动时，你写下的指令就是唯一存在的幽灵。

AI **远比人们**凭直觉想象的更难，恰恰因为你无法只是「告诉」幽灵该做什么。你必须从零开始把幽灵造出来。对你而言显而易见的一切，幽灵都看不到，除非你知道如何让幽灵看到它。你不能只是「告诉」幽灵去看见它。你必须从零开始创造那个「能看见者」。

如果你不知道该如何构建某个看起来带有某些奇怪、不可言说的要素的东西——比如「决策」——那么你就不能耸耸肩，把工作交给幽灵的自由意志。你只能孤零零地、没有幽灵地站在那里。

写一个会下棋的程序，不只是造一个非常快的处理器——这样 AI 就会很聪明——然后在命令提示符里输入：「走**你**认为最好的棋步。」你或许会以为：既然程序员自己棋艺很差，他们给电子超脑的任何建议都只会拖幽灵的后腿。但并不存在幽灵。你看出问题在哪里了吧。

而且，你也没有一种简单咒语可以施展——砰！——把一个完整的幽灵召唤进机器。你不能说：「我召唤了幽灵，它出现了；这就是因果。」（用「[涌现](https://www.readthesequences.com/TheFutilityOfEmergence)」或「[复杂性](https://www.readthesequences.com/SayNotComplexity)」来替代「召唤」这个词也一样没用。）你不能给 CPU 一条指令：「成为一个擅长下棋的人！」你必须洞察下棋思维的奥秘，并从零开始把整个幽灵的结构搭建出来。

无论某件事在你看来多么合乎常识、多么合乎逻辑、多么「显然」或「正确」或「不证自明」或「聪明」，它都不会在幽灵之中发生——**除非**它发生在一条因果链的末端，而那条因果链始于你必须做出的指令选择，并且还包含你在起始指令中植入的、对感官数据的因果依赖。

**这并不意**味着你要[把每一个决策都显式编程进去](https://www.readthesequences.com/ArtificialAddition)。Deep Blue 比它的程序员棋力高得多。Deep Blue 下出的棋步比它的创造者能显式编程进去的任何东西都更好——但并不是因为程序员耸耸肩，把一切交给幽灵。Deep Blue 下得比它的程序员更好……是在一条因果链的末端：那条因果链起始于程序员写下的代码，并从那里依法则推进。没有任何一步是「仅仅因为这一步显然是好棋，所以 Deep Blue 的幽灵般自由意志接管了」而发生的；代码及其合法后果始终在场。

如果你试图洗手不管、放弃约束 AI，你得到的不会是一个像被解放的奴隶那样自由的幽灵。你得到的只会是一堆沙子——还没有人把它提纯成硅、塑造成 CPU，并编程让它思考。

你尽管试着对一块芯片说：「想做什么就做什么！」看看会发生什么？什么也不会发生。因为你甚至没有约束它去理解「自由」。

只要有一个单步是如此明显、如此合乎逻辑、如此不证自明，以至于你的大脑直接跳了过去——你就已经偏离了 AI 程序员应走的道路。要避免大脑这样做，需要付出一种努力，就像我在[把握滑溜之物](https://www.greaterwrong.com/lw/re/grasping_slippery_things/)里所展示的那样。