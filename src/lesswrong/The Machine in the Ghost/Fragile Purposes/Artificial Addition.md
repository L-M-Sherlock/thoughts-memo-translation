# 人工加法

[Artificial Addition](https://www.readthesequences.com/Artificial-Addition)

❦

假设人类完全**不知道**自己是如何进行算术运算的。想象一下，人类具备数羊、把羊相加的能力，是「**进化**」出来的，而不是「**学会**」的。使用这种内建能力的人并不知道它是如何运作的，就像亚里士多德并不知道自己的视觉皮层如何支撑他看见事物的能力一样。我们所熟知的皮亚诺算术（Peano Arithmetic）尚未被发明。有一些哲学家在努力形式化人们的数感，但他们会使用诸如

Plus-Of(Seven, Six) = Thirteen

这样的记号，来形式化这样一个直觉上不证自明的事实：当你把「七」加上「六」时，你当然会得到「十三」。

在这个世界里，袖珍计算器的工作方式，是存储一张巨大的算术事实查找表；这张表由一支专业的人工算术师团队（Artificial Arithmeticians）手工录入，起始值范围从 0 到 100。尽管这些计算器在实践意义上可能有用，但许多哲学家认为，它们只是在「**模拟**」加法，而不是真的在「**做加法**」。机器不可能真的「**计数**」——这就是为什么人类必须先数清 13 只羊，然后再把「十三」输入计算器。计算器可以复述存储的事实，但它永远不可能知道这些陈述是什么意思——如果你输入「二百加二百」，计算器会说「错误：超出范围」。而如果你「**知道**」这些词「**是什么意思**」，那答案**显然**是「四百」。

**当然**，也有一些哲学家不会被这种直觉轻易欺骗。数字其实是一个纯形式系统——「三十七」这个标签之所以有意义，并不是因为这些词本身具有某种内在属性，而是因为这个标签指涉外部世界中的 37 只羊。一个数字之所以获得这种指涉属性，是因为它与其他数字之间由关系构成的语义网络（semantic network）。因此，在计算机程序中，用于表示「三十七」的 LISP 词元并不需要任何内部结构——它之所以有意义，只是因为指涉与关系，而不是因为「三十七」本身的某种计算属性。

从来没有人开发出通用人工算术师（Artificial General Arithmetician），当然，针对特定领域的狭义人工算术师倒是有很多，比如只会处理「二十」到「三十」之间的数字，等等。并且，如果你看看在「二百」这个范围内进展有多慢，你就会清楚：我们短期内不可能得到通用人工算术。该领域最顶尖的专家估计，至少还要 100 年，计算器才能像一个 12 岁人类小孩那样做加法。

但并不是所有人都同意这个估计，也不都认同关于人工算术的[仅凭惯例](https://www.readthesequences.com/TheOutsideTheBoxBox)的看法。人们常会听到类似这样的说法：

-   「这是个框架问题——『二十一加』等于多少，取决于后面是『加三』还是『加四』。只要我们能把足够多的算术事实存储起来，覆盖每个人都知道的常识真理，我们就会开始在网络里看到真正的加法。」

-   「但你永远不可能靠雇专家手工录入来编进那么多算术事实。我们需要的是一种能够『**学习**』数字之间那张巨大关系网络的人工算术师；人类在童年时期通过观察一堆堆苹果获得了这张网络。」

-   「不，我们真正需要的是一种能理解自然语言的人工算术师；这样它就不必被显式告知『二十一加十六等于三十七』，而是可以通过探索 Web 获得知识。」

-   「坦白说，我觉得你们只是在努力说服自己：这个问题能被解决。你们没人真正知道算术是什么，所以你们在用这些泛泛的论证四处乱撞：『我们需要一个能学习 *X* 的 AA』，『我们需要一个能从互联网提取 *X* 的 AA』。我是说，这听起来很好，听起来像是在取得进展，公关上也很好，因为每个人都以为自己理解你们提出的解决方案——但这并不会让你们更接近通用加法，而只是在接近特定领域的加法。也许我们永远也不会知道算术的根本本质。这个问题对人类而言实在太难了。」

-   「所以我们需要像自然那样发展出一个通用算术师——靠进化。」

-   「自上而下的方法显然没能产出算术。我们需要自下而上的方法，需要某种让算术『**涌现**』出来的方式。我们必须承认复杂系统的基本不可预测性。」

-   「你们都错了。过去制造机器算术的努力从一开始就注定徒劳，因为计算力根本不够。看看人类大脑里有多少万亿个突触，就知道计算器的查找表远远没那么大。我们需要与人类大脑同等强大的计算器。按照摩尔定律（Moore’s Law），这将发生在 2031 年 4 月 27 日凌晨 4:00 到 4:30 之间。」

-   「我相信，当研究者把一整个完整人脑的每一个神经元都扫描进计算机，从而模拟人类进行加法的生物电路时，机器算术就会被开发出来。」

-   「我觉得我们不必等到扫描整个人脑。神经网络就像人脑一样，你可以在不知道它们如何做到的情况下训练它们做事。我们会创造出能做算术的程序，而我们——它们的创造者——永远也无需理解它们是如何做算术的。」

-   「但哥德尔定理（Gödel’s Theorem）表明，任何形式系统都不可能捕捉算术的基本性质。经典物理是可形式化的，因此要把二加二算出来，大脑一定利用了量子物理。」

-   「嘿，如果人类算术简单到我们能在计算机里复现，我们就不可能数到足够高，从而造出计算机了。」

-   「你没听说过 John Searle 的 Chinese Calculator Experiment 吗？就算你真的有一套巨大的规则集合，能让你把『二十一』与『十六』相加，只要想象把所有词都翻译成中文，你就能看出其中并没有真正的加法在发生。系统里根本没有任何真实的『**数字**』，只有人类用来表示数字的标签……」

这个寓言不止一个教训，我也曾在不同语境下用不同的寓意讲过它。比如，它说明了组织层级的概念：CPU 之所以能把两个大数相加，是因为数字并不是黑箱般不透明的对象；它们是由 32 位组成的有序结构。

但为了克服偏见，我们在此提炼两条寓意：

-   第一，你相信那些你无法凭自己的知识重新生成出来的断言时，会有危险。

-   第二，你试图围着基本困惑跳舞、打转时，会有危险。

为免有人指责我在[从虚构证据做概括](https://www.readthesequences.com/TheLogicalFallacyOfGeneralizationFromFictionalEvidence)，这两条教训同样可以从人工智能的真实历史中得出。

第一种危险，是 AA 设备在对象层级上遇到的问题：它们像磁带录音机一样，回放由系统之外生成的「知识」，并使用一种它们无法在系统内部捕捉到的过程。一个人可以告诉 AA 设备「二十一加十六等于三十七」，AA 设备可以把这句话记录下来并回放，甚至还可以把「二十一加十六」做模式匹配并输出「三十七！」——但 AA 设备无法为自己生成这样的知识。

这非常类似于：你相信一位物理学家告诉你「[光是波](https://www.readthesequences.com/GuessingTheTeachersPassword)」，你记录下这些迷人的词句；当有人问「光是由什么构成的？」时，你再把它们回放出来——却无法为自己生成这份知识。

第二条寓意，则是吞噬了人工算术研究者与好事旁观者的元层级危险——围着自己知识中的困惑缺口跳舞的危险。那种倾向：几乎愿意做任何事，**唯独**不愿咬紧牙关、俯下身去把那个该死的缺口补上。

无论你说「这是[涌现](https://www.readthesequences.com/SayNotComplexity)出来的！」，还是说「这是[不可知](https://www.readthesequences.com/MysteriousAnswersToMysteriousQuestions)的！」，这两种说法都没有承认：这里需要某个基本洞见；它是可以拥有的，但你尚未拥有。

你怎么知道自己何时会获得一个新的基本洞见？除了拿头去撞问题、尽你所能学习、从尽可能多的角度研究它——也许持续多年——你别无他法。当你每个月至少要发表一篇论文时，学术界并不被设计成能允许这种追求；风投更不可能为此买单。你要么想**现在**就把系统搭出来，要么干脆放弃，转而做别的事。

看看上面的评论：没有一条是在尝试踏上寻找缺失洞见的旅程——那种能让数字不再神秘、让「二十七」不再只是黑箱的洞见。没有任何评论者意识到：他们的困难源于自己头脑中的无知或困惑，而不是算术的内在属性。他们并不是在努力达到这样一种状态：那个令人困惑的东西[不再令人困惑](https://www.readthesequences.com/FailingToLearnFromHistory)。

如果你读过 Judea Pearl 的[《Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference》](https://smile.amazon.com/Probabilistic-Reasoning-Intelligent-Systems-Plausible/dp/1558604790/?sa-no-redirect=1)，[1](#footnote1) 那么你会看到：图模型背后的基本洞见，对于需要它的问题而言是**不可或缺**的。（恐怕它不是那种能印在 T 恤上的东西，所以你得自己去读这本书。我没见过任何在线的贝叶斯网络通俗化能充分传达这些原则背后的理由，或数学为何必须如此这般的重要性；但 Pearl 的书很精彩。）曾经有过几十种「非单调逻辑」，笨拙地试图捕捉这样的直觉：例如「如果我家的防盗警报响了，那可能有窃贼；但如果我随后得知我家附近发生了一次小地震，那就可能不是窃贼」。一旦你拥有图模型这个洞见，你就能用数学解释：为什么一阶逻辑在这类任务上具有错误的性质；并且能用一种紧凑的方式表达正确解法，把所有常识细节一并优雅地打包进去。在获得这个洞见之前，你只能不断地在这里补补逻辑、在那里补补逻辑，添加越来越多的补丁与黑客手段，去强行让它与一切看起来「显然为真」的东西相符合。

在不**知道**关键钥匙之前，你不会「知道」人工算术问题在没有钥匙的情况下是不可解的。如果你不知道规则，你甚至不知道那条规则：要做任何事，你需要知道规则。于是就会出现各种看似可能奏效的聪明点子，例如构建一个能阅读自然语言、并从互联网下载数百万条算术断言的人工算术师。

然而**不知为何**，聪明点子总是行不通。不知为何，你最终总会发现：「我看不出它为什么不行」是因为你对障碍一无所知，而不是因为不存在障碍。这就像蒙着眼睛朝远处的靶子射击——你可以盲射一次又一次，喊着：「你无法证明我打不中靶心！」但在摘下眼罩之前，你甚至还没进入瞄准这项游戏。当「没人能证明」你珍爱的点子不对时，这意味着你并没有足够的信息，去命中一个[巨大答案空间里的小目标](https://www.readthesequences.com/EinsteinsArrogance)。在你知道你的点子会奏效之前，它就不会奏效。

从人工智能历史上那些关键洞见的出现，以及在这些洞见出现之前曾被提出的种种大乱象中，我得出了一个重要的现实教训：当基本问题是你的无知时，试图用聪明策略绕过无知，只会让你搬起石头砸自己的脚。