# 意识：导论

[Minds: An Introduction](https://www.readthesequences.com/Minds-An-Introduction)

作者：Rob Bensinger

你是一个意识，这让你陷入了一种颇为奇特的处境。

意识的存在寥寥无几。你正是宇宙中那一小簇奇特的存在——能够预测未来、制定计划、权衡并修正信念，会痛苦也会做梦，能观察瓢虫，也会突然渴望一颗芒果。你甚至能在意识之中，勾勒出整个自我的图景；能审视自己的思考方式，并努力让它更契合你的目标。

你作为一个意识，依托于人脑而存在。尽管人类大脑拥有惊人的灵活性，但它本质上依然遵循着固有的规律与模式。终其一生，你这个意识可能始终依照某些固定模式运作，却从未察觉这一点。而这些模式会带来深远的影响。

当某种心理模式对你有利时，我们称之为「理性」。

你之所以以如今的模样存在，天生具有某些理性与非理性的特质，皆源于你的祖先。你和地球上的一切生命，都起源于古老的自我复制分子。最初的复制过程笨拙而随机，但很快就在复制体之间产生了可遗传的**差异**。所谓「进化」，就是这些差异随时间推移而发生的变化。

因为其中一些可遗传的差异会影响生存繁衍——这种现象被称为「选择」——进化使得生物逐渐适应了祖先所处的环境。你所有的一切，无不承载着祖先挣扎与胜利留下的印记。

于是就有了此刻的你：一个脱胎于原始本能的意识，试图理解自身的运作方式，以期能够优化它——依照你自己的目标去优化，而非按照塑造你的进化法则。当我们认识到这就是自身存在的基本处境时，又能从中提炼出哪些有益的处世策略与生命洞见呢？

### 幽灵与机器

从细微结构和动态机制来看，我们的大脑与许多机械系统并无二致。然而，我们却很少像理解周遭物体或身体器官那样，去理解自身的意识活动。信念、抉择、语言、观念、情感——这些基本的心智范畴，与我们熟知的物理范畴实则相去甚远。

从前的哲学家基于这一观察进而主张：意识与大脑本质上是两种截然不同、相互分离的现象。这便是哲学家 Gilbert Ryle 所称的「机器中的幽灵教条」[1](https://www.readthesequences.com/Minds-An-Introduction#footnote1)。然而，拒绝二元论的现代科学家与哲学家，并未因此提出更具预测力的意识运作模型。在**实际层面**，我们的目的与欲望仍如游离的幽灵般存在，仿佛一片与科学知识疆域隔绝的领地。我们固然可以谈论“理性”“偏见”与“如何改变想法”，但如果这些概念仍然模糊不清，且缺乏一套统摄性的理论加以约束，那么即便使用看似科学的语言，我们也难免重蹈覆辙——就像那些诉诸灵魂与本质的理论一样陷入谬误。

有趣的是，心灵所笼罩的神秘性与神秘化倾向，不仅妨碍我们看清**人类**自身，也蔓延到了进化生物学和人工智能（AI）中那些看似具有心智或目的性的系统。或许，当我们难以通过直视自身来认识自己时，可以换一面镜子——借助明显非人类的过程，来照见更多真相。

这里有许多幽灵可供我们参详——过去的、现在的、乃至未来的。而这些幻觉本身是真实的认知事件，是我们能够研究和解释的实际现象。倘若机器中看似**有**幽灵，那么这种表象本身，正是机器暗藏机制运作的结果。

《The Machine in the Ghost》的第一篇章[《The Simple Math of Evolution》](https://www.readthesequences.com/The-Simple-Math-Of-Evolution-Sequence)旨在揭示我们的遗传历史、当下生理机制与终极追求之间的错位与冲突。为此，我们需要进行比常见的进化论科普更深入的探讨——那些科普通常只停留在自然选择的表象层面。

第三篇章[《A Human’s Guide to Words》](https://www.readthesequences.com/A-Humans-Guide-To-Words-Sequence)探讨了认知与概念形成之间的基本关系，随后一篇更详尽的文章则系统介绍了贝叶斯推断。

作为这些主题的桥梁，[《Fragile Purposes》](https://www.readthesequences.com/Fragile-Purposes-Sequence)将视角从人类认知与进化，提升至更具普遍意义的心智与目标导向系统。这些文章也意在阐明作者研究哲学与理性科学的基本思路——这一思路深受其在人工智能领域工作的影响。

### 重构智能

Yudkowsky 是一位决策理论家与数学家，专注于通用人工智能（AGI）的基础问题研究——AGI 指涉的是跨领域通用问题解决系统的理论探索。他在人工智能领域的工作，正是驱使他深入探究人类理性心理学的重要动力，正如他在首篇发布于《Overcoming Bias》的博文[《The Martial Art of Rationality》](https://www.greaterwrong.com/lw/gn/the_martial_art_of_rationality/)中所言：

> 我对理性的理解，源于与通用人工智能（AGI）这一难题的交锋（要真正实现它，需要对理性有足够的驾驭力，甚至能用最基础的材料“搭建”出一个完整的理性主体）。从大多数角度来看，AI的挑战远大于个人理性的修炼；但在某些角度上，它反而更简单。关于心智的武术的修习，要求我们学会一种实时调适的能力：准确而及时地操控一台既有的、内部无法修改的大型思维机器。它的部分结构由进化压力塑造而成，与我们使用它时所宣称的目标往往背道而驰。我们固然有意追求真理，但大脑的底层机制却天然倾向于为错误自圆其说。[...]

>

> 想用理性科学来琢磨出一套适合自己的理性修炼方法，听起来可能有点别扭：这就像试图用抽象的物理理论、博弈论和人体解剖学，硬生生编出一门武术来。但人毕竟不是不会反思的机器，我们天生就有自我觉察的本能。虽然我们内心有一双眼睛，但它看得模糊，还总带着各种偏斜。所以我们需要用科学来修正直觉，借助理论知识调整元认知，增强对自己思维的觉察与掌控。我们不是在给木偶编程序让它比划招式——我们要动的，是自己长在脑子里的“手脚”。正因如此，我们必须让理论和实践贯通，真正体会到科学对自己、对每一天的内心世界究竟意味着什么。

以我理解 Yudkowsky 的观点来看：只谈人类理性却对AI毫无洞见，就如同只谈AI却对理性毫无见解一样，终究是说不透的。

从长远看， Yudkowsky 预测人工智能将在一次「智能爆炸」中超越人类——即自我改进的AI提升其有效重构自身的能力，从而引发一连串快速的后续自我升级。「技术奇点」一词有时被用作「智能爆炸」的同义词；在2013年1月之前，MIRI 曾名为「奇点人工智能研究所」，并主办年度奇点峰会。此后，Yudkowsky 开始倾向于使用 I. J. Good 提出的旧术语「智能爆炸」，以将其观点与 Ray Kurzweil 等技术未来学家的「指数进步论」等其他预测区分开来。[2](https://www.readthesequences.com/Minds-An-Introduction#footnote2)

像超越人类智能的人工智能这样的技术，无论带来福祉还是灾难，都极可能引发社会结构的深刻震荡。Yudkowsky 创造了「友好人工智能理论」一术语，用以指代那些旨在使通用人工智能（AGI）的目标与人类偏好保持一致的技术研究。目前，人们对通用智能软件何时得以实现、或在此情况下何种安全路径切实有效，所知依然甚少。即便当下的自主人工智能系统，已难以进行高置信度的验证与确认；而许多现有技术也不太可能直接推广到更智能、更具适应性的未来系统。因此，「友好人工智能」与其说是一套明确的工程目标，不如说更像一系列有待求解的基础数学与哲学问题。

截至2015年， Yudkowsky 对人工智能未来的看法，仍在技术预言家、产业界及学术界的AI研究者中存有争议，尚未形成共识。Nick Bostrom 的著作《Superintelligence》则全景式地梳理了由超越人类智能的AI可能引发的诸多伦理与战略问题。[3](https://www.readthesequences.com/Minds-An-Introduction#footnote3)

对于人工智能领域的通用入门，最广泛采用的教材当属罗素与彼得·诺维格（Peter Norvig）的《人工智能：现代方法》[4](https://www.readthesequences.com/Minds-An-Introduction#footnote4)。在其中探讨人工智能引发的伦理与哲学问题的章节中，罗素与诺维格指出，要为强自适应AI设定良善行为准则，在技术上存在显著困难：

> [Yudkowsky]主张，友好性（即不希望伤害人类）必须从最初就植入系统，但设计者必须认识到，设计本身可能就有缺陷，而系统也会在运行中不断学习和演变。因此，真正的挑战在于设计一套动态机制——让AI系统能在一系列制衡规则下演进，并使其核心目标在面对变化时依然保持友好。我们不能仅仅赋予程序一个一成不变的终极目标，因为环境在变，我们对环境的应对方式也同样在变。

由于担心人工智能、纳米技术、生物技术等领域的未来发展可能威胁人类文明，Bostrom 与 Ćirković  编纂了该主题的首部学术文集《Global Catastrophic Risks》[5](https://www.readthesequences.com/Minds-An-Introduction#footnote5)。其中最严峻的一类被称为「存在性风险」——即可能导致人类社会陷入永久停滞甚至整体灭绝的威胁。

即便是专家，也常常**难以准确预测**重大未来事件（包括新技术的发展）。Yudkowsky 探讨理性的部分目的，正是要厘清哪些认知偏差阻碍了我们预见重大变革并提前做好准备的能力。他在《Global Catastrophic Risks》文集中贡献的两篇文章——[《Cognitive biases potentially affecting judgement of global risks》](https://intelligence.org/files/CognitiveBiases.pdf)与[《Artificial intelligence as a positive and negative factor in global risk》](https://intelligence.org/files/AIPosNegFactor.pdf)，将他在认知科学与人工智能领域的研究紧密结合。此外，Yudkowsky  与 Bostrom 在《Cambridge Handbook of Artificial Intelligence》的 [《The ethics of artificial intelligence》](http://www.nickbostrom.com/ethics/artificial-intelligence.pdf)[7](https://www.readthesequences.com/Minds-An-Introduction#footnote7)章节中，一并梳理了近期与长远的担忧。

虽然本书探讨的是**人类**理性，但人工智能作为一个能简明呈现人类认知某些侧面的领域，依然具有重要参考价值。长期技术预测也是贝叶斯理性的一项重要应用——它能在数据稀少或含义模糊的领域中，依然构建出合理的推理框架。

了解设计，便能推知设计者的用心；了解设计者，亦能领悟设计的深意。

那么，就让我们从追问开始：我们的创造者，能让我们怎样认识自己？