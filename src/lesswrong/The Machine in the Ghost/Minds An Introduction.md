# 心智：导论

[Minds: An Introduction](https://www.readthesequences.com/Minds-An-Introduction)

作者：Rob Bensinger

你是一个心智，这让你陷入了一种颇为奇特的困境。

极少有事物能成为心智。你正是宇宙中那一小簇奇特的存在——能够预测未来、制定计划、权衡并修正信念，会痛苦也会做梦，会注意到瓢虫，也会突然渴望一颗芒果。你甚至能在心智之中，勾勒出整个心智的图景。你能推理自己的推理过程，并努力让它更契合你的目标。

你是一个心智，依托于人脑而存在。而事实证明，尽管人脑拥有惊人的灵活性，它却是一个有规律可循之物，一个由模式和常规构成之物。你的心智可能一生遵循某种固定模式，却从未察觉这一点。而这些模式会带来深远的影响。

当某种思维模式对你有利时，我们称之为「理性」。

你之所以以如今的模样存在——天生具有某些理性与非理性的特质——皆源于你的祖先。你，以及地球上的一切生命，都起源于古老的自复制分子。最初的复制过程笨拙而随机，但很快就在复制体之间产生了可复制的**差异**。所谓「演化」，就是这些差异随时间推移而发生的变化。

因为其中一些可复制的差异会影响生存繁衍——这种现象被称为「选择」——演化使得生物逐渐适应了祖先所处的环境。你所有的一切，无不承载着祖先挣扎与胜利的印记。

于是就有了此刻的你：一个心智，脱胎于更弱的心智，试图理解你自身的运作方式，以期能够优化它——按照你自己的目标去优化，而非按照塑造你的演化法则。认识到这就是我们的基本处境后，我们能从中提炼出哪些有用的处世策略与生命洞见呢？

### 幽灵与机器

从细微结构和动态机制来看，我们的大脑与许多机械系统并无二致。然而，我们却很少像看待周遭物体或身体器官那样，去看待我们的心智。信念、决策、语言、观念、情感——这些基本的心智范畴，与我们熟知的物理范畴相去甚远。

从前的哲学家基于这一观察大做文章，主张心智与大脑是两种本质上截然不同、相互分离的现象。这便是哲学家 Gilbert Ryle 所称的「机器中的幽灵教条」[1](https://www.readthesequences.com/Minds-An-Introduction#footnote1)。然而，拒绝二元论的现代科学家与哲学家，未必提出了更具预测力的心智运作模型取而代之。在**实践层面**，我们的目的与欲望仍如游离的幽灵般存在，仿佛一片与科学知识疆域隔绝的领地。我们固然可以谈论「理性」「偏见」以及「如何改变我们的想法」，但如果这些概念仍然模糊不清，且缺乏一套统摄性的理论加以约束，那么即便使用看似科学的语言，我们也难免重蹈覆辙——就像那些诉诸灵魂与本质的理论一样陷入谬误。

有趣的是，笼罩着心智的神秘与迷雾，不仅妨碍我们看清**人类**自身，也蔓延到了演化生物学和人工智能（AI）中那些看似具有心智或目的性的系统。或许，当我们难以通过审视自身来认识自己时，可以换一面镜子——借助明显**非**人类的过程，来照见更多真相。

这里有许多幽灵可供我们参详——过去的、现在的、乃至未来的。而这些幻觉本身是真实的认知事件，是我们能够研究和解释的真实现象。倘若机器中**似乎**有幽灵，那么这「似有」之相本身，便是机器隐藏运作的结果。

《幽灵中的机器》的第一章[《演化的简单数学》](https://www.readthesequences.com/The-Simple-Math-Of-Evolution-Sequence)旨在传达我们的遗传历史、当下的生物学状态与终极追求之间的冲突与分歧。为此，我们需要进行比面向非生物学家的演化论入门更深入的探讨——那些入门通常只停留在自然选择的表象层面。

第三章[《人类词语指南》](https://www.readthesequences.com/A-Humans-Guide-To-Words-Sequence)探讨了认知与概念形成之间的基本关系，随后是一篇介绍贝叶斯推断的长文。

作为这些主题间的桥梁，[《脆弱的目的》](https://www.readthesequences.com/Fragile-Purposes-Sequence)从人类认知与进化中抽象出来，探讨最一般意义下的心智与目标导向系统。这些文章也承载着第二重目的：解释作者研究哲学与理性科学的总体方法——这一方法深受其在人工智能领域工作的影响。

### 重构智能

Yudkowsky 是一位决策理论家与数学家，从事通用人工智能（AGI）——跨领域通用问题解决系统的理论——的基础问题研究。Yudkowsky在AI（人工智能）领域的工作，正是他探究人类理性心理学的主要驱动力，正如他在《克服偏见》上的第一篇博文[《理性的武术》](https://www.greaterwrong.com/lw/gn/the_martial_art_of_rationality/)中所言：

> 我对理性的理解，源自与通用人工智能这一难题的交锋（要真正实现它，需要对理性有足够的驾驭力，以至于能用最简单的材料构建出一个完整可运行的理性主体）。在大多数方面，AI问题的挑战远大于个人理性的修炼；但在某些方面，它反而更简单。在心智的武术修习中，我们需要学会一种实时调适的能力，用以准确而及时地操控一台既有的、内部无法修改的大型思维机器。它的部分结构由演化选择压力塑造而成，而这些压力与我们宣称使用它的目标背道而驰。我们固然有意只追求真理，但大脑的底层机制却天然倾向于为错误自圆其说。[...]

>

> 想用理性科学来琢磨出一套适合自己的理性修炼方法，可能会显得笨拙：这就像试图用抽象的物理理论、博弈论和人体解剖学，硬生生发明出一门武术来。但人毕竟不是不会反思的机器，我们天生就有自我觉察的本能。我们内心有一双眼睛，但它视线模糊，还总带着系统性的扭曲。因此，我们需要用科学来修正直觉，借助抽象的知识来纠正思维，并增强对自己元认知的掌控。我们不是在给木偶编程序让它比划招式——我们要动的，是自己长在脑子里的“手脚”。正因如此，我们必须让理论和实践贯通，真正体会到科学对我们自身、对我们每一天的内心世界究竟意味着什么。

以我理解 Yudkowsky 的观点来看：只谈人类理性却对AI毫无洞见，就如同只谈AI却对理性毫无见解一样，终究是说不透的。

从长远看， Yudkowsky 预测AI将在一次「智能爆炸」中超越人类——到那时，自我改进的AI会提升其有效重构自身的能力，从而引发一连串快速的后续自我升级。「技术奇点」一词有时被用作「智能爆炸」的同义词；在2013年1月之前，MIRI 还被命名为「奇点人工智能研究所」，并主办年度奇点峰会。此后，Yudkowsky 开始倾向于使用 I. J. Good 提出的旧术语「智能爆炸」，以将其观点与其他未来主义预测区分开来，例如 Ray Kurzweil 的「指数式技术进步论」。[2](https://www.readthesequences.com/Minds-An-Introduction#footnote2)

诸如超越人类智能的AI之类的技术，无论带来福祉还是灾难，都可能引发巨大的社会动荡。Yudkowsky 创造了「友好AI理论」一术语，指代旨在使通用人工智能的偏好与人类偏好保持一致的技术研究。目前，对于通用智能软件何时得以实现、或在此情况下何种安全路径切实有效，人们依然知之甚少。如今已经很难对自主AI进行高置信度的验证与确认，而且许多现有技术也不太可能推广到更智能、更具适应性的系统。因此，「友好AI」与其说是一套明确规定的编程目标，不如说更像一系列基础数学与哲学问题的集合。

截至2015年， 产业界和学术界的技术预测者和AI研究者仍对 Yudkowsky 关于AI未来的观点存有争议，尚未达成共识。Nick Bostrom 的著作《超级智能》（《Superintelligence》）则宏观地总结了由超越人类智能的AI引发的诸多伦理与战略问题。[3](https://www.readthesequences.com/Minds-An-Introduction#footnote3)

对于AI领域的通用入门，最广泛采用的教材当属罗素（Bertrand Russell）与诺维格（Peter Norvig）的《人工智能：现代方法》[4](https://www.readthesequences.com/Minds-An-Introduction#footnote4)。在其中探讨AI引发的伦理与哲学问题的章节中，罗素与诺维格指出了为强自适应AI设定良善行为准则的技术困难：

> [Yudkowsky]主张，友好性（即不希望伤害人类）必须从最初就植入系统，但设计者必须认识到，设计本身可能就有缺陷，而系统也会在运行中不断学习和演变。因此，真正的挑战在于设计一套动态机制——让AI系统能在一系列制衡规则下演进，并使其核心目标在面对变化时依然保持友好。我们不能仅仅赋予程序一个一成不变的指令，因为环境在变，我们对环境的应对方式也同样在变。

由于担心AI、纳米技术、生物技术等领域的未来发展可能威胁人类文明，Bostrom 与 Ćirković  编纂了关于该主题的首部学术文集《全球灾难性风险》（《Global Catastrophic Risks》）[5](https://www.readthesequences.com/Minds-An-Introduction#footnote5)。其中最极端的一类被称为「存在性风险」，即可能导致人类永久停滞或灭绝的风险。

人们（包括专家）往往**极不擅长预测**重大未来事件（包括新技术）。Yudkowsky 探讨理性的部分目的，正是要厘清哪些认知偏差干扰了我们预见重大动荡并提前做好准备的能力。他在《全球灾难性风险》文集中贡献的两篇文章——[《可能影响全球风险判断的认知偏见》（《Cognitive biases potentially affecting judgement of global risks》）](https://intelligence.org/files/CognitiveBiases.pdf)与[《人工智能作为全球风险中的正面与负面因素》（《Artificial intelligence as a positive and negative factor in global risk》）](https://intelligence.org/files/AIPosNegFactor.pdf)——将他在认知科学与人工智能领域的研究紧密结合。此外，Yudkowsky  与 Bostrom 在《剑桥人工智能手册》（《Cambridge Handbook of Artificial Intelligence》）中的 [《人工智能伦理》（《The ethics of artificial intelligence》）](http://www.nickbostrom.com/ethics/artificial-intelligence.pdf)[7](https://www.readthesequences.com/Minds-An-Introduction#footnote7)一章里，总结了近期与长远的担忧。

虽然本书探讨的是**人类**理性，但AI作为一个能简明呈现人类认知某些方面的领域，依然具有参考价值。长期技术预测也是贝叶斯理性的一项重要应用——它能在数据稀缺或模棱两可的领域中，依然构建出合理的推理框架。

了解设计，便能推知设计者的用心；了解设计者，亦能领悟设计的深意。

那么，就让我们从追问开始：我们的创造者，能教会我们关于自身的什么？