# 邮票收集者

原文：[The Stamp Collector](https://mindingourway.com/the-stamp-collector/)

2015 年 4 月 26 日

从前，有一群天真的哲学家发现了一个会收集小物件的机器人。更确切地说，这个机器人似乎专注于收集邮票：如果你给这个机器人提供多种小物件作为选择，它总会选择那个能让它的库存中邮票数量最大化的选项。它对骰子、瓶盖、易拉罐、树棍、细枝等等都视而不见，除非它预期这些物件在一两个回合之后能换成邮票。因此，哲学家们自然而然地称它为「邮票收集者」。

后来，有一天，哲学家们发现了计算机，进而推断出所谓的「邮票收集者」不过是一个在其头部内置的处理器中运行的软件程序。然而，这个程序对他们而言太过复杂，难以完全理解，但他们还是成功设法推断出，机器人只有少数几个传感器（位于其眼睛上和其库存内部），并利用这些传感器来构建世界模型。

其中一位哲学家感到困惑，说：「嘿，等一下，这东西其实根本不是一个真正的邮票收集者。如果这个机器人只是在其头部构建一个世界模型，那么它就不可能是在优化（属于）其自身的实际库存，因为它根本接触不到那份真实的邮票库存。它从始至终只能根据头脑中重构的世界模型行动！\*」（\*译者注：也就是说，这个机器人只能知道也只能操作内部模型中虚拟的邮票库存，但虚拟库存的滞后性和片面性会导致其与真实库存脱节，从而无法成功实现真实的邮票收集）

「啊，是的，我明白了，」另一个哲学家回答他，「我们称它为『邮票收集者』，这个叫法对它并不恰当。显然，这个机器人并没有真正接触到外部世界，因为通过传感器来观察世界，并在其核心处理器中构建一个模型。因此，它**实际上**肯定不是在最大化其库存中的邮票数量。那是不可能的，因为它的邮票库存在其核心处理器之外。更确切地说，它肯定是在最大化其核心处理器中的『内部邮票计数器』。」

于是，这群天真的哲学家们对此心满意足地点了点头，然后便不再琢磨那个「邮票收集者」是如何工作的了。

------

这一推理中存在诸多缺陷。首先，这些天真的哲学家犯了「小人谬误」（homunculus error）。机器人程序或许无法「真正接触」到其库存中邮票的数量（姑且不论「真正接触」究竟何指），但它**同样**也无法「真正接触」到其内在的邮票计数器。

机器人并非被某个能主宰其内部机制、却无法主宰其外部环境的「小人」所占据！这个抽象程序，对存放邮票计数器的寄存器的访问，与对其库存的访问，并非一个是「真实的」一个是「虚假的」。引导现实朝向库存中有大量邮票的状态，与引导现实朝向邮票计数寄存器呈现高数值模式的状态，本质上是**同一种事**。并不存在那么一个随心所欲画出来的『魔法圈』，能恰好把内存圈在其内，同时把库存排除其外，好让里面的『小人』全权做主；机器人程序对于所谓『真实硬件』的访问程度，和它对于所谓『真实邮票』的访问程度，实际上是一样间接和有限的。

这就引出了他们推理中的第二个缺陷：试图用选择本身（或类似选择的东西）来解释选择。你不能通过说「因为墙是由微小的红色原子组成的」来解释墙为什么是红色的；这根本不构成对「红色」的**解释**。要解释红色，你必须借助非红色的事物。然而，人类有一种坏习惯，就是用事物本身来解释那些令人困惑的事物。为何活着的肉体能响应精神指令，而死去的肉体不能？嗨，因为活着的肉体蕴含着[生命力](http://en.wikipedia.org/wiki/Élan_vital)（Élan Vital）。我们那些天真的哲学家犯了同样的错误：他们说，「它怎么可能选择那些让库存中有更多邮票的结果呢？啊哈！那肯定是靠选择那些让邮票计数器数值更高的结果来实现的！」，如此一来，他们是用选择解释了选择，而非用更基础的原理来解释。

说「它试图往其库存中收集邮票，是因为它试图最大化其邮票计数器」**这算不上一种解释**。一个真正的解释应该更像是这样：机器人的计算机运行一个程序，该程序利用传感器数据来构建一个世界模型。该世界模型包含一项对其库存中邮票数量的表征。然后，该程序会遍历一系列可选行动，预测（根据模型）采取该行动后其库存中将会有多少邮票，并输出那个预计能使其拥有最多邮票的行动。

我们**也**可以假设机器人包含这样一个程序：它模拟世界，预测每个行动将如何改变世界，**然后**预测**该**结果将如何影响内部存储器的某个特定位置，**再然后**选择那个能最大化该内部计数器的行动。这是可能的！你完全可以造出那样的机器！但这只是一个严格来说更复杂的假说，因此其复杂性本身就会让它在评估时处于劣势，但至少，它还算是一种解释！

而且，幸运的是，这还是一个**可检验的**解释：我们可以看看，当机器人面临直接增加邮票计数器寄存器读数（而不实际增加其拥有的邮票数量）的机会时，它会怎么做。让我们拭目以待，看看我们那些天真的哲学家对此会有何反应……

------

> 嘿，你们看：我找到了机器人内存里的邮票计数器。我读不了它的数值，但我确实找到了一个增加它数值的方法。所以我给了机器人以下选项：拿一张邮票，或者不拿邮票但我把它的邮票计数器加十。猜猜它选了哪个？

「嗯，显而易见，它会选后者！」一位天真的哲学家立刻回答道。

> 不对！它选了前者。

「……哈！这意味着，**拒绝**让邮票计数器被动手脚所带来的『邮票度』（stampyness），肯定价值超过 10 张邮票！」

> 啊？什么是「邮票度」？

「嗨，『邮票度』嘛，就是机器人内部衡量*采取某个特定行动*能在多大程度上增加其邮票计数器的一种度量。」

> 什么？太荒谬了。我很确定它就只是在收集邮票。

「不可能！程序无法访问它实际拥有多少邮票；那是外部世界的属性。机器人**必然**是依据其核心处理器中实际存在的值来进行优化的。」

> 来，我们试试给它提供这些选项：要么我给它一张邮票，要么我把它的邮票计数器增加到阿克曼函数(g64, g64)那么多——哦看，它拿了邮票。

「哇！那可是个天文数字，所以这几乎可以肯定，拒绝这件事本身的所带来的『邮票度』是取决于它所拒绝的『邮票度』有多大！它现在肯定高兴坏了，因为你提供了一个如此诱人、让它得以拒绝的提议，从而给了它**巨量**的『邮票度』。」

> 哦，对了，你们看，我刚找到一个把邮票计数器设到最大值的方法。来，我试试给它一个选择，要么（a）一张邮票，要么（b）我把它的邮票计数器设到最——哦，看，它已经拿了邮票了。

「难以置信！那一定意味着还有某个其他的计数器在测量**微**-『邮票度』，也就是它在选择行动时**立刻**获得的『邮票度』，在你来得及修改之前！啊，是的，这必定是它拒绝你将其计数器设为最大值的唯一合理解释了，它**必然**是根据它感知到的每个可选行动所能带来的即时**微**-『邮票度』来做选择的！干得漂亮，我的老伙计，我们今天可真是收获颇丰啊！」

------

啊！不！让我们把这一点说得非常清楚：机器人是在预测哪些行动会带来哪些**结果**，它对这些结果进行排序，并采取那些能导向最佳结果的行动。[行动是依据其**成果**来评价的。行动**本身**并无内在价值！](http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/)

你看到这些天真的哲学家错在哪儿了吗？他们假设假设了这个智能体会将**行动**视为**目的**，并试图趋向其最偏好的任何**行动**——仿佛行动本身就是终极目的。

你不能智能体是根据「采取某个行动好不好」来对行动排序，以此解释它为何采取某个行动。这就回避了问题本身：到底哪些行动是「好」的？

这个智能体将那些能导向「智能体库存中有大量邮票」的结果的行动评定为「好」的。行动是依据它们所达成的结果来评价的；它们本身并无内在价值。

机器人程序并不包含现实世界，但它也无需包含。它仍然能够**影响**现实世界。如果它的世界模型与现实世界相关联，并且它采取那些（根据其预测）能带来更多**实际**邮票的行动，那么它就会倾向于积累邮票。

它**并非**试图将未来导向那些它碰巧选择了最具「微邮票度」行动的情境；它只是在将未来导向那些它预测自己将实际拥有更多邮票的世界。

------

现在，让我给你讲讲第二个故事：

从前，一群天真的哲学家遇到了一群人类。人类似乎总是选择那些能给他们带来快乐的行动。有时他们享用美食，有时他们体验性爱，有时他们赚钱以便日后花在能带来快乐的事情上，但（至少在最初几周）他们总是采取那些能带来快乐的行动。

但后来有一天，其中一个人把大笔钱捐给了一家慈善机构。

「这怎么可能呢？」哲学家们问道，「人类是快乐最大化者啊！」他们思索片刻后说：「啊，一定是他们从把钱捐给慈善机构中获得的快乐，超过了他们若把钱花掉所能得到的快乐。」

再后来，一位母亲为了救自己的孩子，纵身跳到了一辆车前。

天真的哲学家们目瞪口呆，直到突然有人喊道：「我明白了！**选择那个行动**所带来的即时微快乐，一定超过了——

------

人们会告诉你，人类总是做且仅仅只做那些给他们带来快乐的事情。人们会告诉你，世上根本没有利他主义这回事，人们永远只做自己想做的事。

人们会告诉你，因为我们被困在自己的头脑（认知）里，所以我们只能关心头脑内部的事物，比如我们自身的欲求和渴望。

但我有话要告诉你：事实上，你完全可以去关心外在的世界。

而且，你也能引导它。只要你愿意。